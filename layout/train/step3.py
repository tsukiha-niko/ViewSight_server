import csv
import os
import shutil
import time
from concurrent.futures import ThreadPoolExecutor

import pandas as pd
import requests

# API é…ç½®
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
API_KEYS = [
    "sk-hdilnusolwkldxlyvzmjzaqkobspqdrorncuteywvqbuvlhw",  # è€çš„ API_KEY
    "sk-jagtlkdxrfrdiyoucgqqimndcvqfkrujvkvfjztevzgomojo"  # æ–°çš„ API_KEY
]
TREND_MODEL_ID = "Pro/deepseek-ai/DeepSeek-V3"


# è·å– Bilibili çƒ­é—¨è§†é¢‘æ ‡é¢˜
def fetch_hot_titles(max_retries=3, delay=2):
    url = "https://api.bilibili.com/x/web-interface/popular?Ps=100"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Referer": "https://www.bilibili.com/",
        "Origin": "https://www.bilibili.com"
    }
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            data = response.json()
            if data["code"] == 0 and "list" in data["data"]:
                titles = [item["title"] for item in data["data"]["list"]]
                return titles
            else:
                print(f"âŒ è·å– Bilibili çƒ­ç‚¹å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return []
        except requests.exceptions.RequestException as e:
            print(f"âŒ è·å– Bilibili çƒ­ç‚¹å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(delay)  # åœ¨é‡è¯•å‰ç­‰å¾…
            else:
                return []


# AI è¶‹åŠ¿åˆ†æå‡½æ•°ï¼ˆå•æ‰¹å¤„ç†ï¼‰
def analyze_trending_relevance(titles, comments, hot_titles, api_key_idx):
    prompt = (
        "è¯·åˆ†æä»¥ä¸‹10ä¸ªè§†é¢‘æ ‡é¢˜å’Œå°é¢è¯„è®ºçš„è¶‹åŠ¿ç›¸å…³æ€§ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿”å›ç»“æœï¼š\n"
        "1. **ä»…è¿”å›çº¯å­—ç¬¦ä¸²ï¼Œä¸åŒ…å«ä»»ä½• JSON æ ‡è®°ã€å¤šä½™æ–‡æœ¬ã€æ³¨é‡Šæˆ–è¯´æ˜**ã€‚\n"
        "2. è¿”å›10ç»„è¯„åˆ†ï¼Œæ¯ç»„å¯¹åº”ä¸€ä¸ªæ ‡é¢˜å’Œå°é¢è¯„è®ºå¯¹ï¼Œç”¨ '|' åˆ†éš”æ¯ç»„è¯„åˆ†ã€‚\n"
        "3. æ¯ç»„è¯„åˆ†åŒ…å«4ä¸ªå­—æ®µï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œé¡ºåºä¸ºï¼štrending emotion visual creativityã€‚\n"
        "4. æ¯ä¸ªå­—æ®µå€¼ä¸º0åˆ°1ä¹‹é—´çš„å°æ•°ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰ã€‚\n"
        "   - trending: æ—¶åŠ¿çƒ­ç‚¹ï¼ˆæ ‡é¢˜å’Œå°é¢æ˜¯å¦ç´§è·Ÿå½“å‰çƒ­ç‚¹ï¼Œç»“åˆæµè¡Œè¯é¢˜ã€äº‹ä»¶æˆ–è¶‹åŠ¿ï¼Œå¸å¼•å…³æ³¨ï¼‰ã€‚\n"
        "   - emotion: æƒ…æ„Ÿå…±é¸£ï¼ˆæ ‡é¢˜å’Œå°é¢èƒ½å¦å¼•èµ·æƒ…æ„Ÿååº”ï¼Œå¦‚å¥½å¥‡ã€å…´å¥‹ã€æ„ŸåŠ¨ã€æç¬‘ï¼Œä¿ƒè¿›ç‚¹å‡»ï¼‰ã€‚\n"
        "   - visual: è§†è§‰å¸å¼•åŠ›ï¼ˆå°é¢è®¾è®¡æ˜¯å¦å¸å¼•ï¼Œèƒ½å¦åœ¨å¹³å°ä¸­è„±é¢–è€Œå‡ºï¼‰ã€‚\n"
        "   - creativity: åˆ›æ„ä¸ç‹¬ç‰¹æ€§ï¼ˆæ ‡é¢˜å’Œå°é¢æ˜¯å¦ç‹¬ç‰¹ï¼Œèƒ½ä¸å…¶ä»–å†…å®¹åŒºåˆ†ï¼Œæä¾›æ–°é²œæ„Ÿï¼‰ã€‚\n"
        "5. å¦‚æœè¾“å…¥å°‘äº10ä¸ªæ ‡é¢˜å’Œè¯„è®ºå¯¹ï¼Œä»éœ€è¿”å›10ç»„è¯„åˆ†ï¼Œæœªä½¿ç”¨çš„ç»„ç”¨ '0.00 0.00 0.00 0.00' å¡«å……ã€‚\n"
        "6. ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼Œæ‰€æœ‰æ•°å€¼åœ¨ 0-1 èŒƒå›´å†…ï¼Œæ— éæ³•å­—ç¬¦ï¼Œè¯·ä½ åœ¨è¯„ä»·æ—¶å¸¦ä¸Šæ›´å¤šçš„æ€è€ƒå’Œå®¢è§‚æ€§ï¼Œè®©è¯„åˆ†å·®è·å®¢è§‚ç»†è‡´ã€‚\n"
        f"å½“å‰çƒ­ç‚¹æ ‡é¢˜ï¼ˆå‚è€ƒç”¨ï¼‰ï¼š{', '.join(hot_titles[:30])}\n"
        f"è¾“å…¥æ•°æ®ï¼š{str([{'title': t, 'comment': c} for t, c in zip(titles, comments)])}\n"
        "ç¤ºä¾‹è¾“å‡ºï¼š\n"
        "0.86 0.77 0.97 0.83 | 0.65 0.67 0.71 0.64 | 0.35 0.25 0.47 0.92 | ...ï¼ˆå…±10ç»„ï¼‰"
    )
    payload = {
        "model": TREND_MODEL_ID,
        "stream": False,
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.9,
        "top_k": 40,
        "frequency_penalty": 0.5,
        "n": 1,
        "stop": [],
        "messages": [
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }
        ]
    }
    headers = {
        "Authorization": f"Bearer {API_KEYS[api_key_idx % len(API_KEYS)]}",
        "Content-Type": "application/json"
    }
    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=90)
        if response.status_code == 200:
            content = response.json()["choices"][0]["message"]["content"].strip()
            try:
                groups = content.split("|")
                if len(groups) != 10:
                    raise ValueError(f"è¿”å›çš„è¯„åˆ†ç»„æ•°ä¸æ˜¯10: {len(groups)}")
                result = []
                for group in groups:
                    values = group.strip().split()
                    if len(values) != 4:
                        raise ValueError(f"è¯„åˆ†å­—æ®µæ•°ä¸æ˜¯4: {group}")
                    scores = [float(v) for v in values]
                    if not all(0 <= s <= 1 for s in scores):
                        raise ValueError(f"è¯„åˆ†è¶…å‡ºèŒƒå›´: {group}")
                    result.append(
                        {"trending": scores[0], "emotion": scores[1], "visual": scores[2], "creativity": scores[3]})
                return result
            except (ValueError, IndexError) as e:
                print(f"âŒ è¾“å‡ºæ ¼å¼é”™è¯¯: {content}ï¼Œé”™è¯¯: {e}")
                return [{"trending": 0.00, "emotion": 0.00, "visual": 0.00, "creativity": 0.00}] * 10
        else:
            print(f"âŒ API è¿”å›é”™è¯¯ [{response.status_code}]ï¼š{response.text}")
            return [{"trending": 0.00, "emotion": 0.00, "visual": 0.00, "creativity": 0.00}] * 10
    except Exception as e:
        print(f"âŒ API åˆ†æå¼‚å¸¸ï¼š{e}")
        return [{"trending": 0.00, "emotion": 0.00, "visual": 0.00, "creativity": 0.00}] * 10


# å¤„ç†å•æ‰¹æ•°æ®ï¼ˆçº¿ç¨‹ä»»åŠ¡ï¼‰
def process_batch(batch_data):
    df, start_idx, hot_titles, batch_idx = batch_data
    batch_titles = df["è§†é¢‘æ ‡é¢˜"][start_idx:start_idx + 10].tolist()
    batch_comments = df["å°é¢è¯„è®º"][start_idx:start_idx + 10].tolist()
    result = analyze_trending_relevance(batch_titles, batch_comments, hot_titles, batch_idx)
    return start_idx, result


# é‡æ’åº CSVï¼Œå¼‚å¸¸æ•°æ®ç§»åˆ°æœ«å°¾
def reorder_csv(csv_path):
    df = pd.read_csv(csv_path, encoding="utf-8-sig")
    normal_data = df[
        (df["æ’­æ”¾æ•°"].fillna(0) != 0) &
        (~df["å°é¢è¯„è®º"].isna()) &
        (~df["å°é¢è¯„è®º"].str.contains("é”™è¯¯|åˆ†æå¤±è´¥", na=False))
        ]
    abnormal_data = df[
        (df["æ’­æ”¾æ•°"].fillna(0) == 0) |
        (df["å°é¢è¯„è®º"].isna()) |
        (df["å°é¢è¯„è®º"].str.contains("é”™è¯¯|åˆ†æå¤±è´¥", na=False))
        ]
    reordered_df = pd.concat([normal_data, abnormal_data], ignore_index=True)
    reordered_df.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC, encoding="utf-8-sig")
    print(f"âœ… CSV å·²é‡æ’åºï¼Œå¼‚å¸¸æ•°æ®ç§»è‡³æœ«å°¾ï¼š{csv_path}")


# ä¸»é€»è¾‘
def process_csv(csv_path, original_csv_path):
    hot_titles = fetch_hot_titles()
    print(f"âœ… è·å–åˆ° {len(hot_titles)} ä¸ª Bilibili çƒ­é—¨è§†é¢‘æ ‡é¢˜")

    if not hot_titles:  # å¦‚æœæ²¡æœ‰è·å–åˆ°çƒ­ç‚¹æ ‡é¢˜ï¼Œé€€å‡ºç¨‹åº
        print("âŒ æœªè·å–åˆ°çƒ­ç‚¹æ ‡é¢˜ï¼Œç¨‹åºé€€å‡º")
        return

    df = pd.read_csv(csv_path, encoding="utf-8-sig")
    if "è§†é¢‘æ ‡é¢˜" not in df.columns or "å°é¢è¯„è®º" not in df.columns:
        print("âŒ CSV ç¼ºå°‘å¿…è¦åˆ—ï¼šè§†é¢‘æ ‡é¢˜æˆ–å°é¢è¯„è®º")
        return

    # åˆå§‹åŒ–å››åˆ—ï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸º float
    for col in ["trending", "emotion", "visual", "creativity"]:
        if col not in df.columns:
            df[col] = pd.NA
        df[col] = pd.to_numeric(df[col], errors="coerce").astype("Float64")

    total_rows = len(df)
    batch_size = 10
    completed_count = 0

    # ä½¿ç”¨ 64 çº¿ç¨‹å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=64) as executor:
        batches = [(df, start_idx, hot_titles, i) for i, start_idx in enumerate(range(0, total_rows, batch_size))]
        future_to_batch = {executor.submit(process_batch, batch): batch for batch in batches}

        for future in future_to_batch:
            start_idx, result = future.result()
            print(f"\nğŸ“¥ å¤„ç†ç¬¬ {start_idx + 1} åˆ° {min(start_idx + batch_size, total_rows)} è¡Œ")
            for i, json_result in enumerate(result):
                row_idx = start_idx + i
                if row_idx < total_rows:
                    df.at[row_idx, "trending"] = json_result["trending"]
                    df.at[row_idx, "emotion"] = json_result["emotion"]
                    df.at[row_idx, "visual"] = json_result["visual"]
                    df.at[row_idx, "creativity"] = json_result["creativity"]

            df.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC, encoding="utf-8-sig")
            print(f"ğŸ’¾ æ‰¹æ¬¡å·²ä¿å­˜åˆ°ç¼“å­˜ CSV: {csv_path}")

            completed_count += batch_size
            if completed_count % 100 == 0:
                shutil.copy(csv_path, original_csv_path)
                print(f"ğŸ“ å·²å¤„ç† {completed_count} è¡Œï¼ŒåŒæ­¥åˆ°åŸå§‹æ–‡ä»¶: {original_csv_path}")

    df.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC, encoding="utf-8-sig")
    shutil.copy(csv_path, original_csv_path)
    print(f"âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {original_csv_path}")

    reorder_csv(original_csv_path)


if __name__ == "__main__":
    original_csv = r"C:\Users\ElmCose\Desktop\æ‰“å°\temp\merged.csv"
    cache_csv = "cache_merged.csv"

    if os.path.exists(original_csv):
        shutil.copy(original_csv, cache_csv)
        print(f"ğŸš€ å¼€å§‹å¤„ç† CSVï¼ˆç¼“å­˜æ–‡ä»¶ï¼š{cache_csv}ï¼‰...")
        process_csv(cache_csv, original_csv)
        print("âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    else:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{original_csv}")
