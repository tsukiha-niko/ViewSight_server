import base64
import concurrent.futures
import csv
import io
import os
import shutil

import pandas as pd
import requests
from PIL import Image

# ====================== API é…ç½® ======================
API_URL = "https://api.siliconflow.cn/v1/chat/completions"
API_KEYS = [
    "sk-hdilnusolwkldxlyvzmjzaqkobspqdrorncuteywvqbuvlhw",  # è€çš„ API_KEY
    "sk-jagtlkdxrfrdiyoucgqqimndcvqfkrujvkvfjztevzgomojo"  # æ–°çš„ API_KEY
]
MODEL_ID = "Qwen/Qwen2.5-VL-32B-Instruct"


# ====================== å›¾åƒå¤„ç†å‡½æ•° ======================
def download_image(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return Image.open(io.BytesIO(response.content)).convert("RGB")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{url}ï¼Œé”™è¯¯ï¼š{e}")
        return None


def prepare_image_for_model(image, target_size_kb=10, max_resolution=(256, 256), color_reduction=64):
    def resize_image(image, max_size):
        image.thumbnail(max_size, Image.Resampling.LANCZOS)
        return image

    def reduce_colors(image, colors):
        return image.convert("P", palette=Image.ADAPTIVE, colors=colors).convert("RGB")

    def compress_webp(image, target_size_kb, min_quality=10):
        best_image = None
        best_size = float('inf')
        for quality in range(95, min_quality - 1, -5):
            buffer = io.BytesIO()
            image.save(buffer, format="WEBP", quality=quality)
            size_kb = buffer.tell() / 1024
            if size_kb <= target_size_kb:
                buffer.seek(0)
                return Image.open(buffer).convert("RGB")
            if size_kb < best_size:
                best_size = size_kb
                best_image = buffer
        print(f"âš ï¸ æ— æ³•å‹ç¼©åˆ° {target_size_kb}KBï¼Œå·²é€‰ç”¨æœ€å°ç‰ˆæœ¬ï¼ˆ{best_size:.2f}KBï¼‰")
        best_image.seek(0)
        return Image.open(best_image).convert("RGB")

    image = resize_image(image, max_resolution)
    image = reduce_colors(image, color_reduction)
    image = compress_webp(image, target_size_kb)
    return image


# ====================== AI åˆ†æå‡½æ•°ï¼ˆè°ƒç”¨ APIï¼‰ ======================
def analyze_image(image, api_key_idx):
    try:
        buffer = io.BytesIO()
        image.save(buffer, format="JPEG")
        img_b64 = base64.b64encode(buffer.getvalue()).decode("utf-8")
        image_data_uri = f"data:image/jpeg;base64,{img_b64}"
        prompt = (
            "è¯·ç®€è¦åˆ†æè¿™å¼ è§†é¢‘å°é¢ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼šå›¾ä¸­æ˜¯å¦æœ‰æ ‡é¢˜æ–‡å­—ï¼Ÿå¦‚æœæœ‰ï¼Œè¯·è¯´å‡ºæ˜¯ä»€ä¹ˆæ–‡å­—å¹¶å‘Šè¯‰æˆ‘ä»–çš„å¤§å°æ ·å¼ï¼›æ²¡æœ‰åˆ™è¯´æ˜æ— æ–‡å­—ã€‚"
            "ç„¶åæå…¶çš„ç®€æ˜æ‰¼è¦çš„åˆ†æç”»é¢çš„æ„å›¾ã€è‰²å½©é£æ ¼ã€å¸ç›å…ƒç´ å’Œæ•´ä½“æ„Ÿè§‰ã€‚æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚ä¸éœ€è¦ä½¿ç”¨markdownå’Œåˆ†æ®µè¯´æ˜ï¼Œåªå›ç­”ä¸€æ®µæ®µè½çº¯æ–‡æœ¬å³å¯"
        )
        payload = {
            "model": MODEL_ID,
            "stream": False,
            "max_tokens": 512,
            "temperature": 0.7,
            "top_p": 0.7,
            "top_k": 50,
            "frequency_penalty": 0.5,
            "n": 1,
            "stop": [],
            "messages": [
                {"role": "user", "content": [
                    {"type": "image_url", "image_url": {"url": image_data_uri}},
                    {"type": "text", "text": prompt}
                ]}
            ]
        }
        headers = {
            "Authorization": f"Bearer {API_KEYS[api_key_idx % len(API_KEYS)]}",  # äº¤æ›¿ä½¿ç”¨ API_KEY
            "Content-Type": "application/json"
        }
        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
        if response.status_code != 200:
            print(f"âŒ API è¿”å›é”™è¯¯ [{response.status_code}]ï¼š{response.text}")
            return "åˆ†æå¤±è´¥"
        result = response.json()
        reply = result["choices"][0]["message"]["content"].strip()
        return reply if reply else "åˆ†æå¤±è´¥"
    except Exception as e:
        print(f"âŒ API åˆ†æå¼‚å¸¸ï¼š{e}")
        return "åˆ†æå¤±è´¥"


# ====================== å•è¡Œå¤„ç†å‡½æ•° ======================
def process_row(idx, cover_url, output_folder):
    print(f"\nğŸ“¥ å¤„ç†ç¬¬ {idx + 1} è¡Œï¼Œå°é¢é“¾æ¥ï¼š{cover_url}")
    image = download_image(cover_url)
    if image is None:
        return idx, "ä¸‹è½½å¤±è´¥"
    image = prepare_image_for_model(image)
    image_path = os.path.join(output_folder, f"cover_{idx + 1}.jpg")
    try:
        image.save(image_path)
    except Exception as e:
        print(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥ï¼š{image_path}ï¼Œé”™è¯¯ï¼š{e}")
    print("ğŸ§  æ­£åœ¨åˆ†æ...")
    # ä½¿ç”¨ idx å¯¹ API_KEYS å–æ¨¡ï¼Œäº¤æ›¿åˆ†é…
    analysis = analyze_image(image, idx)
    if analysis.lower() in ["åˆ†æå¤±è´¥", ""]:
        print("âš ï¸ åˆ†æå¤±è´¥ï¼Œä¿ç•™ç©ºç™½ä»¥ä¾¿ä¸‹æ¬¡é‡è¯•ã€‚")
    else:
        print(f"âœ… è¾“å‡ºï¼š{analysis}")
    return idx, analysis


# ====================== ä¸»é€»è¾‘ ======================
def process_csv(csv_path, original_csv_path):
    df = pd.read_csv(csv_path, encoding="utf-8-sig")
    if "å°é¢è¯„è®º" not in df.columns:
        df["å°é¢è¯„è®º"] = pd.NA

    output_folder = "compressed_images"
    os.makedirs(output_folder, exist_ok=True)

    completed_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:  # æå‡è‡³ 20 çº¿ç¨‹
        future_to_idx = {}
        for idx, row in df.iterrows():
            cover_url = row.get("å°é¢")
            current_comment = row.get("å°é¢è¯„è®º")

            if pd.isna(cover_url):
                continue
            if not (pd.isna(current_comment) or str(current_comment).strip() in ["", "ä¸‹è½½å¤±è´¥", "åˆ†æå¤±è´¥"]):
                continue

            future = executor.submit(process_row, idx, cover_url, output_folder)
            future_to_idx[future] = idx

        for future in concurrent.futures.as_completed(future_to_idx):
            idx, result = future.result()
            df.at[idx, "å°é¢è¯„è®º"] = result

            # æ¯æ¬¡å¤„ç†å®Œä¸€è¡Œï¼Œä¿å­˜åˆ° cache_merged.csv
            if completed_count % 10 == 0:
                try:
                    df.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC, encoding="utf-8-sig")
                    print(f"ğŸ’¾ ç¬¬ {idx + 1} è¡Œå·²å†™å…¥ç¼“å­˜ CSV: {csv_path}")
                except Exception as e:
                    print(f"âŒ å†™å…¥ç¼“å­˜ CSV å¤±è´¥: {e}")

            completed_count += 1
            if completed_count % 100 == 0:
                # æ¯ 100 æ¬¡å°è¯•åŒæ­¥åˆ° merged.csv
                try:
                    shutil.copy(csv_path, original_csv_path)
                    print(f"ğŸ“ å·²å¤„ç† {completed_count} æ¡ï¼ŒåŸå§‹æ–‡ä»¶åŒæ­¥æ›´æ–°: {original_csv_path}")
                except Exception as e:
                    print(f"âŒ åŒæ­¥åˆ°åŸå§‹æ–‡ä»¶å¤±è´¥: {e}ï¼Œä¿ç•™ç¼“å­˜æ–‡ä»¶: {csv_path}")

    try:
        df.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC, encoding="utf-8-sig")
        df.to_csv(original_csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC, encoding="utf-8-sig")
        print(f"âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶: {csv_path}")
    except Exception as e:
        print(f"âŒ æœ€ç»ˆä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")


if __name__ == "__main__":
    original_csv = r"C:\Users\ElmCose\Desktop\æ‰“å°\temp\merged.csv"
    cache_csv = "cache_merged.csv"

    # å¦‚æœ merged.csv å­˜åœ¨ï¼Œå¤åˆ¶åˆ° cache_merged.csv ä½œä¸ºåˆå§‹å·¥ä½œæ–‡ä»¶
    if os.path.exists(original_csv):
        try:
            shutil.copy(original_csv, cache_csv)
            print(f"ğŸš€ å¼€å§‹å¤„ç† CSVï¼ˆç¼“å­˜æ–‡ä»¶ï¼š{cache_csv}ï¼‰...")
            process_csv(cache_csv, original_csv)
            print("âœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“Œ æœ€ç»ˆç»“æœä¿å­˜åœ¨: {original_csv}")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æˆ–å¤„ç†å¤±è´¥: {e}")
    else:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{original_csv}")
